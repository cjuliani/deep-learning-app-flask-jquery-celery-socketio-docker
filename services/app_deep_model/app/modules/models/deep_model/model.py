import os
import tensorflow as tf

import warnings
with warnings.catch_warnings():
    warnings.simplefilter("ignore")

from tensorflow.keras import layers, models
from modules.models import metrics


class Model(tf.keras.Model):
    """The learning model consisting of method for train step,
    optimization, prediction, loss and evaluation metrics.

    Attributes:
        model (object): the learning model.
        trainable (bool): if True, current model class is set to
            trainable.
        num_classes (int): number of classes considered (set as 2).
        inner_weight (float): weight applied to inner model weights
            (generated by e.g. regularization).
        cce: the categorical cross-entropy loss function.
        optimizer: the function optimizing the model parameters
            during the learning process.
        precision: the metric function used to calculate
            precision from.
        recall: the metric function used to calculate
            recall from.
        specificity: the metric function used to calculate
            specificity from.
    """

    def __init__(self, in_shape, lr, metric_interval, inner_weight,
                 num_classes=1, trainable=True, **kwargs):
        super(Model, self).__init__(**kwargs)

        self.model = build_vgg19(
            inp_shape=in_shape,
            num_classes=num_classes)
        self.model.trainable = trainable
        self.trainable = True
        self.num_classes = num_classes
        self.inner_weight = inner_weight
        self.cce = tf.keras.losses.SparseCategoricalCrossentropy(
            from_logits=False,
            reduction=tf.keras.losses.Reduction.NONE,
            name='cross_entropy')
        self.optimizer = tf.keras.optimizers.Adam(learning_rate=lr)
        self.precision = metrics.Precision(name="precision", interval=metric_interval, multiclass=False)
        self.recall = metrics.Recall(name="recall", interval=metric_interval, multiclass=False)
        self.specificity = metrics.Specificity(name="specificity", interval=metric_interval, multiclass=False)

    def __call__(self, inputs, y_true, *args, **kwargs):
        """Returns the losses and metrics given the input tensors.

        Args:
            inputs (tensor): the input tensors to the learning model.
            y_true (tensor): reference data to learn from (labels).
        """
        # Call model.
        out_cls, _ = self.model(inputs)

        # Calculate loss.
        loss = self.cce(y_true=y_true, y_pred=out_cls)
        loss = tf.reduce_mean(loss)

        # Segmentation metrics.
        metrics_obj = self.get_metrics(
            y_true=tf.one_hot(tf.cast(y_true, tf.int32), depth=self.num_classes),
            y_pred=tf.one_hot(tf.argmax(out_cls, axis=-1), depth=self.num_classes))
        metric_values = [metric.result() for metric in metrics_obj]
        metric_names = [metric.name for metric in metrics_obj]

        # Total loss.
        inner_losses = (tf.reduce_sum(self.model.losses) + tf.reduce_sum(self.losses))
        tot_loss = loss + (inner_losses * self.inner_weight)

        return tot_loss, loss, inner_losses, metric_values, metric_names

    def optimize(self, model_args, var_list):
        """Returns the losses and metrics given the input tensors,
        after calling the learning model, calculating the gradients
        and optimizing the model parameters based on the gradients.

        Args:
            model_args (list): the input arguments to the learning model.
            var_list (list): variables of the learning model considered for
                optimization.
        """
        # GradientTape allows watching model variables and recording
        # operations to perform automatic differentiation.
        with tf.GradientTape() as tape:
            outputs = self.__call__(*model_args)
            (tot_loss, loss, inner_losses,
             metric_values, metric_names) = outputs

            # Get gradients from variables
            grads = tape.gradient(tot_loss, var_list)

            # Update parameters
            self.optimizer.apply_gradients(
                (grad, var) for (grad, var) in zip(grads, var_list) if grad is not None)

        return tot_loss, loss, inner_losses, metric_values, metric_names

    def train_step(self, model_args, optimize):
        """Returns loss vector after or without optimization.

        Args:
            model_args (list or dict or tensor): the input arguments to the learning model.
            optimize (bool): if True, apply the optimization.
        """
        if optimize:
            outputs = self.optimize(model_args, self.trainable_variables)
            (tot_loss, loss, inner_losses,
             metric_values, metric_names) = outputs
        else:
            outputs = self.__call__(*model_args)
            (tot_loss, loss, inner_losses,
             metric_values, metric_names) = outputs

        loss_vec = tf.concat([[tot_loss], [loss], [inner_losses]], axis=0)
        return loss_vec, metric_values, metric_names

    def load_variables(self, directory, show_info=True, msg_handler=None):
        """Loads weights of variables from the learning model."""
        filename = self.model.name
        filepath = os.path.join(directory, filename)
        self.model.load_weights(filepath)

        msg = f"'{filename}' weights loaded."
        if msg_handler: msg_handler(msg, show_info)

        # Load weights of current wrapping model.
        filename = self.name
        filepath = os.path.join(directory, filename)
        self.load_weights(filepath)

        msg = f"'{filename}' weights loaded."
        if msg_handler: msg_handler(msg, show_info)

    def save_variables(self, directory):
        """Saves weights of variables from the learning model."""
        filename = self.model.name
        filepath = os.path.join(directory, filename)
        self.model.save_weights(filepath)

        # Save weights of current wrapping model.
        filename = self.name
        filepath = os.path.join(directory, filename)
        self.save_weights(filepath)

    def get_metrics(self, y_pred, y_true):
        """Returns the metric values associated to the model.

        Args:
            y_pred (tensor): prediction data to estimate.
            y_true (tensor): reference data to learn from.
        """
        cls_true_ = tf.cast(y_true, tf.float32)
        cls_pred_ = tf.cast(y_pred, tf.float32)
        self.recall.update_state(y_pred=cls_pred_, y_true=cls_true_)
        self.precision.update_state(y_pred=cls_pred_, y_true=cls_true_)
        self.specificity.update_state(y_pred=cls_pred_, y_true=cls_true_)
        return [self.specificity, self.recall, self.precision]

    def predict(self, inputs, weights_to_restore=None, save_path=None,
                batch_size=1):
        """Returns the classes predicted for the input batch.

        Args:
            inputs (tensor): the input tensor.
            weights_to_restore (str or None): name of the model
                whose weights must be restored.
            save_path (str or None): path to save the model.
            batch_size (int): size of batch to process.
        """
        if weights_to_restore:
            # Path to model.
            directory_path = os.path.join(save_path, weights_to_restore)
            # Load weights and other variables.
            self.load_variables(directory=directory_path)
        # Predict from model.
        out_cls, _ = self.model.predict(inputs, batch_size=batch_size)
        return tf.argmax(out_cls, axis=-1)


def conv(inp, filters, kernel_size, stride=1, padding="same", act=None, name=None):
    """Returns convolution results on input tensor."""
    x = layers.Conv2D(
        filters=filters,
        kernel_size=(kernel_size, kernel_size),
        strides=stride,
        padding=padding,
        kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-2, l2=1e-2),
        name=name if name else None)(inp)
    x = layers.BatchNormalization(
        epsilon=1.001e-5,
        momentum=0.95)(x)
    if act is None:
        return x
    else:
        return layers.Activation(act)(x)


def classifier(inp, num_classes):
    """Returns the classification results as probability values.

    Args:
        inp (keras.layers.layer): the input keras layer.
        num_classes (int): the output number of classes to be predicted.
    """
    x = layers.BatchNormalization(
        axis=-1,
        epsilon=1.001e-5,
        name='dec_cls_bn1')(inp)

    x = layers.Activation('relu')(x)
    x = layers.Conv2D(
        filters=16,
        kernel_size=(3, 3),
        strides=2,
        padding='same',
        activation='relu',
        kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-2, l2=1e-2),
        name='dec_cls_conv1')(x)

    x = layers.Flatten(name='dec_rec_flatten')(x)
    x = layers.Dense(
        256,
        activation="relu",
        name='dec_cls_dense1')(x)

    x = layers.Dense(
        128,
        activation="relu",
        name='dec_cls_dense2')(x)

    x = layers.Dense(
        64,
        activation="relu",
        name='dec_cls_dense3')(x)

    return layers.Dense(
        num_classes,
        activation="softmax",
        name='dec_cls_out')(x)  # (B, num_classes)


def residual_identity(inp, filters, name):
    """Residual identity block.

    Source:
        https://arxiv.org/abs/1512.03385
    """
    f1, f2 = filters
    x = conv(inp, filters=f1, kernel_size=1, stride=1, padding='valid', act="relu", name=name + "_ident_conv1")
    x = conv(inp=x, filters=f1, kernel_size=3, stride=1, padding='same', act="relu", name=name + "_ident_conv2")
    x = conv(inp=x, filters=f2, kernel_size=1, stride=1, act=None, padding='valid', name=name + "_ident_conv3")
    return tf.keras.layers.Activation('relu')(tf.add(x, inp))


def residual_convolution(inp, filters, stride, name):
    """Residual convolution block.

    Source:
        https://arxiv.org/abs/1512.03385
    """
    f1, f2 = filters
    x = conv(inp=inp, filters=f1, kernel_size=1, stride=stride, padding='valid', act="relu", name=name + "_res_conv1")
    x = conv(inp=x, filters=f1, kernel_size=3, stride=1, padding='same', act="relu", name=name + "_conv2")
    x = conv(inp=x, filters=f2, kernel_size=1, stride=1, act=None, padding='valid', name=name + "_res_conv3")
    x_skip = conv(inp, filters=f2, kernel_size=1, stride=stride, act=None, padding='valid', name=name + "_res_conv4")
    return tf.keras.layers.Activation('relu')(tf.add(x, x_skip))


def build_resnet50(inp_shape, num_classes):
    """Returns a classifier model with ResNet50 as backbone network.

    Source:
        https://arxiv.org/abs/1512.03385
    """
    # Check input shape.
    H, W, channels = inp_shape
    assert ((H % 16) == 0) and ((W % 16) == 0), "Spatial dimensions must be divisible by 4."

    # Build model.
    inputs = layers.Input(shape=(H, W, channels))

    x = conv(inp=inputs, filters=4, kernel_size=7, stride=2, padding='valid', act="relu", name="init_conv")
    x = tf.keras.layers.MaxPooling2D((3, 3), (2, 2), padding='same', name='pool1')(x)

    x = residual_convolution(x, (4, 32), 1, "res1")
    x = residual_identity(x, (4, 32), "id1")
    x = residual_identity(x, (4, 32), "id2")

    x = residual_convolution(x, (16, 64), 2, "res2")
    x = residual_identity(x, (16, 64), "id3")
    x = residual_identity(x, (16, 64), "id4")
    x = residual_identity(x, (16, 64), "id5")

    x = residual_convolution(x, (32, 128), 2, "res3")
    x = residual_identity(x, (32, 128), "id6")
    x = residual_identity(x, (32, 128), "id7")
    x = residual_identity(x, (32, 128), "id8")
    x = residual_identity(x, (32, 128), "id9")
    x = residual_identity(x, (32, 128), "id10")

    x = residual_convolution(x, (64, 256), 2, "res4")
    x = residual_identity(x, (64, 256), "id11")
    x = residual_identity(x, (64, 256), "id12")

    # Classifier.
    out_CLS = classifier(x, num_classes)

    # Create and return model.
    return models.Model(inputs, [out_CLS, x], name="resnet50")


def build_vgg19(inp_shape, num_classes):
    """Returns a classifier model with VGG19 as backbone network.

    Source:
        https://arxiv.org/abs/1409.1556
    """
    # Check the input shape.
    H, W, channels = inp_shape
    assert ((H % 16) == 0) and ((W % 16) == 0), "Spatial dimensions must be divisible by 4."

    # Build model.
    inputs = layers.Input(shape=(H, W, channels))

    x = conv(inp=inputs, filters=64, kernel_size=3, stride=1, padding='same', act="relu", name="conv11")
    x = conv(inp=x, filters=64, kernel_size=3, stride=1, padding='same', act="relu", name="conv12")
    x = tf.keras.layers.MaxPooling2D((2, 2), (2, 2), padding='same', name='pool1')(x)

    x = conv(inp=x, filters=128, kernel_size=3, stride=1, padding='same', act="relu", name="conv21")
    x = conv(inp=x, filters=128, kernel_size=3, stride=1, padding='same', act="relu", name="conv22")
    x = tf.keras.layers.MaxPooling2D((2, 2), (2, 2), padding='same', name='pool2')(x)

    x = conv(inp=x, filters=256, kernel_size=3, stride=1, padding='same', act="relu", name="conv31")
    x = conv(inp=x, filters=256, kernel_size=3, stride=1, padding='same', act="relu", name="conv32")
    x = conv(inp=x, filters=256, kernel_size=3, stride=1, padding='same', act="relu", name="conv33")
    x = conv(inp=x, filters=256, kernel_size=3, stride=1, padding='same', act="relu", name="conv34")
    x = tf.keras.layers.MaxPooling2D((2, 2), (2, 2), padding='same', name='pool3')(x)

    x = conv(inp=x, filters=512, kernel_size=3, stride=1, padding='same', act="relu", name="conv41")
    x = conv(inp=x, filters=512, kernel_size=3, stride=1, padding='same', act="relu", name="conv42")
    x = conv(inp=x, filters=512, kernel_size=3, stride=1, padding='same', act="relu", name="conv43")
    x = conv(inp=x, filters=512, kernel_size=3, stride=1, padding='same', act="relu", name="conv44")
    x = tf.keras.layers.MaxPooling2D((2, 2), (2, 2), padding='same', name='pool4')(x)

    x = conv(inp=x, filters=512, kernel_size=3, stride=1, padding='same', act="relu", name="conv51")
    x = conv(inp=x, filters=512, kernel_size=3, stride=1, padding='same', act="relu", name="conv52")
    x = conv(inp=x, filters=512, kernel_size=3, stride=1, padding='same', act="relu", name="conv53")
    x = conv(inp=x, filters=512, kernel_size=3, stride=1, padding='same', act="relu", name="conv54")

    # Classifier.
    out_CLS = classifier(x, num_classes)

    # Create and return model.
    return models.Model(inputs, [out_CLS, x], name="vgg19")
